{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0659ff06",
   "metadata": {},
   "source": [
    "Lecture: AI I - Basics \n",
    "\n",
    "Previous:\n",
    "[**Chapter 3.5: Preprocessing with Pandas**](../05_preprocessing.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590d441",
   "metadata": {},
   "source": [
    "# Exercise 3.5: Preprocessing with Pandas\n",
    "\n",
    "- [Task 1 - Cleaning and Preparation of Medical and Geographic Data](#task-1---cleaning-and-preparation-of-medical-and-geographic-data)\n",
    "- [Task 2 - Data Preparation of the Titanic Dataset](#task-2---data-preparation-of-the-titanic-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b0b0e",
   "metadata": {},
   "source": [
    "> Hint: When doing the exercises put your solution in the designated \"Solution\" section:\n",
    "> ```python\n",
    "> # Solution (put your code here)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2cdd1",
   "metadata": {},
   "source": [
    "## Task 1 - Cleaning and Preparation of Medical and Geographic Data\n",
    "\n",
    "In this week's exercise, we try to clean up a medical dataset and recombine another geographic dataset. The medical dataset is a report on the number of cases and deaths in various countries due to Ebola. You can find the dataset in `ebola_country_timeseries.csv`. The `Date` column is a reference to the date of observation. The `Day` column is a reference to the day of observation, starting at 0 with the first observation. All other columns contain the number of cases or deaths due to Ebola for a specific country.\n",
    "\n",
    "a) **Get familiar with the dataset (without automatic evaluation):** This is the first step you should do for any dataset you work with. Load the dataset with Pandas. Look at its `head()`, its columns, and let pandas `describe()` the dataset for you. If you want to know something about individual columns, you can also use `value_counts()` for them, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062f4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f13a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_Guinea",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_Liberia",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_SierraLeone",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_Nigeria",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_Senegal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_UnitedStates",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_Spain",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cases_Mali",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_Guinea",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_Liberia",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_SierraLeone",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_Nigeria",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_Senegal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_UnitedStates",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_Spain",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Deaths_Mali",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "81a2b590-7dbd-4d6d-9916-dd1f19aafc5b",
       "rows": [
        [
         "count",
         "122",
         "122.0",
         "93.0",
         "83.0",
         "87.0",
         "38.0",
         "25.0",
         "18.0",
         "16.0",
         "12.0",
         "92.0",
         "81.0",
         "87.0",
         "38.0",
         "22.0",
         "18.0",
         "16.0",
         "12.0"
        ],
        [
         "unique",
         "122",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "top",
         "1/5/2015",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "freq",
         "1",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "mean",
         null,
         "144.77868852459017",
         "911.0645161290323",
         "2335.3373493975905",
         "2427.367816091954",
         "16.736842105263158",
         "1.08",
         "3.2777777777777777",
         "1.0",
         "3.5",
         "563.2391304347826",
         "1101.20987654321",
         "693.7011494252873",
         "6.131578947368421",
         "0.0",
         "0.8333333333333334",
         "0.1875",
         "3.1666666666666665"
        ],
        [
         "std",
         null,
         "89.31645959894857",
         "849.1088014277038",
         "2987.9667214863625",
         "3184.803995702903",
         "5.9985773562337075",
         "0.4",
         "1.1785113019775793",
         "0.0",
         "2.7468990781342053",
         "508.51134534255254",
         "1297.208567617881",
         "869.9470729014646",
         "2.7819014752365305",
         "0.0",
         "0.38348249442368526",
         "0.4031128874149275",
         "2.4058010698889443"
        ],
        [
         "min",
         null,
         "0.0",
         "49.0",
         "3.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "29.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0"
        ],
        [
         "25%",
         null,
         "66.25",
         "236.0",
         "25.5",
         "64.5",
         "15.0",
         "1.0",
         "3.0",
         "1.0",
         "1.0",
         "157.75",
         "12.0",
         "6.0",
         "4.0",
         "0.0",
         "1.0",
         "0.0",
         "1.0"
        ],
        [
         "50%",
         null,
         "150.0",
         "495.0",
         "516.0",
         "783.0",
         "20.0",
         "1.0",
         "4.0",
         "1.0",
         "2.5",
         "360.5",
         "294.0",
         "334.0",
         "8.0",
         "0.0",
         "1.0",
         "0.0",
         "2.0"
        ],
        [
         "75%",
         null,
         "219.5",
         "1519.0",
         "4162.5",
         "3801.0",
         "20.0",
         "1.0",
         "4.0",
         "1.0",
         "6.25",
         "847.75",
         "2413.0",
         "1176.0",
         "8.0",
         "0.0",
         "1.0",
         "0.0",
         "6.0"
        ],
        [
         "max",
         null,
         "289.0",
         "2776.0",
         "8166.0",
         "10030.0",
         "22.0",
         "3.0",
         "4.0",
         "1.0",
         "7.0",
         "1786.0",
         "3496.0",
         "2977.0",
         "8.0",
         "0.0",
         "1.0",
         "1.0",
         "6.0"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Cases_Guinea</th>\n",
       "      <th>Cases_Liberia</th>\n",
       "      <th>Cases_SierraLeone</th>\n",
       "      <th>Cases_Nigeria</th>\n",
       "      <th>Cases_Senegal</th>\n",
       "      <th>Cases_UnitedStates</th>\n",
       "      <th>Cases_Spain</th>\n",
       "      <th>Cases_Mali</th>\n",
       "      <th>Deaths_Guinea</th>\n",
       "      <th>Deaths_Liberia</th>\n",
       "      <th>Deaths_SierraLeone</th>\n",
       "      <th>Deaths_Nigeria</th>\n",
       "      <th>Deaths_Senegal</th>\n",
       "      <th>Deaths_UnitedStates</th>\n",
       "      <th>Deaths_Spain</th>\n",
       "      <th>Deaths_Mali</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>122</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>25.00</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1/5/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>144.778689</td>\n",
       "      <td>911.064516</td>\n",
       "      <td>2335.337349</td>\n",
       "      <td>2427.367816</td>\n",
       "      <td>16.736842</td>\n",
       "      <td>1.08</td>\n",
       "      <td>3.277778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>563.239130</td>\n",
       "      <td>1101.209877</td>\n",
       "      <td>693.701149</td>\n",
       "      <td>6.131579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>3.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>89.316460</td>\n",
       "      <td>849.108801</td>\n",
       "      <td>2987.966721</td>\n",
       "      <td>3184.803996</td>\n",
       "      <td>5.998577</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.178511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.746899</td>\n",
       "      <td>508.511345</td>\n",
       "      <td>1297.208568</td>\n",
       "      <td>869.947073</td>\n",
       "      <td>2.781901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383482</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>2.405801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>157.750000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>516.000000</td>\n",
       "      <td>783.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>360.500000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>219.500000</td>\n",
       "      <td>1519.000000</td>\n",
       "      <td>4162.500000</td>\n",
       "      <td>3801.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>847.750000</td>\n",
       "      <td>2413.000000</td>\n",
       "      <td>1176.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>2776.000000</td>\n",
       "      <td>8166.000000</td>\n",
       "      <td>10030.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1786.000000</td>\n",
       "      <td>3496.000000</td>\n",
       "      <td>2977.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date         Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone  \\\n",
       "count        122  122.000000     93.000000      83.000000          87.000000   \n",
       "unique       122         NaN           NaN            NaN                NaN   \n",
       "top     1/5/2015         NaN           NaN            NaN                NaN   \n",
       "freq           1         NaN           NaN            NaN                NaN   \n",
       "mean         NaN  144.778689    911.064516    2335.337349        2427.367816   \n",
       "std          NaN   89.316460    849.108801    2987.966721        3184.803996   \n",
       "min          NaN    0.000000     49.000000       3.000000           0.000000   \n",
       "25%          NaN   66.250000    236.000000      25.500000          64.500000   \n",
       "50%          NaN  150.000000    495.000000     516.000000         783.000000   \n",
       "75%          NaN  219.500000   1519.000000    4162.500000        3801.000000   \n",
       "max          NaN  289.000000   2776.000000    8166.000000       10030.000000   \n",
       "\n",
       "        Cases_Nigeria  Cases_Senegal  Cases_UnitedStates  Cases_Spain  \\\n",
       "count       38.000000          25.00           18.000000         16.0   \n",
       "unique            NaN            NaN                 NaN          NaN   \n",
       "top               NaN            NaN                 NaN          NaN   \n",
       "freq              NaN            NaN                 NaN          NaN   \n",
       "mean        16.736842           1.08            3.277778          1.0   \n",
       "std          5.998577           0.40            1.178511          0.0   \n",
       "min          0.000000           1.00            1.000000          1.0   \n",
       "25%         15.000000           1.00            3.000000          1.0   \n",
       "50%         20.000000           1.00            4.000000          1.0   \n",
       "75%         20.000000           1.00            4.000000          1.0   \n",
       "max         22.000000           3.00            4.000000          1.0   \n",
       "\n",
       "        Cases_Mali  Deaths_Guinea  Deaths_Liberia  Deaths_SierraLeone  \\\n",
       "count    12.000000      92.000000       81.000000           87.000000   \n",
       "unique         NaN            NaN             NaN                 NaN   \n",
       "top            NaN            NaN             NaN                 NaN   \n",
       "freq           NaN            NaN             NaN                 NaN   \n",
       "mean      3.500000     563.239130     1101.209877          693.701149   \n",
       "std       2.746899     508.511345     1297.208568          869.947073   \n",
       "min       1.000000      29.000000        2.000000            0.000000   \n",
       "25%       1.000000     157.750000       12.000000            6.000000   \n",
       "50%       2.500000     360.500000      294.000000          334.000000   \n",
       "75%       6.250000     847.750000     2413.000000         1176.000000   \n",
       "max       7.000000    1786.000000     3496.000000         2977.000000   \n",
       "\n",
       "        Deaths_Nigeria  Deaths_Senegal  Deaths_UnitedStates  Deaths_Spain  \\\n",
       "count        38.000000            22.0            18.000000     16.000000   \n",
       "unique             NaN             NaN                  NaN           NaN   \n",
       "top                NaN             NaN                  NaN           NaN   \n",
       "freq               NaN             NaN                  NaN           NaN   \n",
       "mean          6.131579             0.0             0.833333      0.187500   \n",
       "std           2.781901             0.0             0.383482      0.403113   \n",
       "min           0.000000             0.0             0.000000      0.000000   \n",
       "25%           4.000000             0.0             1.000000      0.000000   \n",
       "50%           8.000000             0.0             1.000000      0.000000   \n",
       "75%           8.000000             0.0             1.000000      0.000000   \n",
       "max           8.000000             0.0             1.000000      1.000000   \n",
       "\n",
       "        Deaths_Mali  \n",
       "count     12.000000  \n",
       "unique          NaN  \n",
       "top             NaN  \n",
       "freq            NaN  \n",
       "mean       3.166667  \n",
       "std        2.405801  \n",
       "min        1.000000  \n",
       "25%        1.000000  \n",
       "50%        2.000000  \n",
       "75%        6.000000  \n",
       "max        6.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdcd317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore the dataset using head(), columns, describe(), and value_counts() on individual columns.\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "# No automatic test for this part - explore the data yourself!\n",
    "print(\"Explore the dataset using head(), columns, describe(), and value_counts() on individual columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b05d4",
   "metadata": {},
   "source": [
    "b) **Fill missing values:** The Ebola dataset contains many missing values for dates when the number of cases and deaths could not be determined. Write a function `fill_nans(ebola_data)` to fix this. Since we are dealing with a time series, we assume that the value simply remains the same as the last date a measurement was obtained. First sort the values from early to late day. Then use the forward fill method to fill in the missing values. There will be some missing values left at the top. Fill these with the median of the respective column. Look at the lecture again on how to do this easily using Pandas primitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13231d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill missing values test passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14805/314609508.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  ebola_data_filled = ebola_data_sorted.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "df = pd.read_csv('../data/preprocessing/ebola_country_timeseries.csv')\n",
    "result = fill_nans(df)\n",
    "\n",
    "assert isinstance(result, pd.DataFrame), 'Your function needs to return a DataFrame.'\n",
    "expected = pd.read_csv('../data/preprocessing/ebola_no_nans.csv', index_col=0)\n",
    "assert_frame_equal(result.reset_index(drop=True).sort_index(axis=1),\n",
    "                   expected.reset_index(drop=True).sort_index(axis=1))\n",
    "print(\"Fill missing values test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e1b3f",
   "metadata": {},
   "source": [
    "c) **Convert column headers to variables:** Write a function `tidy(ebola_data_no_nans)` to clean the ebola dataset. Your function receives the dataframe which should be the output of your function from task 1. However, this is stored in the file `ebola_no_nans.csv` so you don't \"have to\" solve task 1 to complete task 2.\n",
    "\n",
    "**Hint:** The Ebola dataset is a classic example of a dataset that uses column headers as a place to store variables. Let's combat this abuse in two steps. First, merge all columns except `Date` and `Day` into a `value` column called `count` and a `variable` column. Then split the new `variable` column into a `status` and `country` column. Delete the old `variable` column that was created during merging. Finally, sort the dataframe. First by `Day`, then by `country` and by `status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd76229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f2aab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidy data test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "df = pd.read_csv('../data/preprocessing/ebola_no_nans.csv', index_col=0)\n",
    "expected = pd.read_csv('../data/preprocessing/ebola_tidy.csv', index_col=0)\n",
    "result = tidy(df)\n",
    "assert isinstance(result, pd.DataFrame), 'Your function needs to return a DataFrame.'\n",
    "assert_frame_equal(result.reset_index(drop=True).sort_index(axis=1),\n",
    "                   expected.reset_index(drop=True).sort_index(axis=1))\n",
    "print(\"Tidy data test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f24a6",
   "metadata": {},
   "source": [
    "d) **Merging:** This task focuses on another dataset. We try again to find population densities, but for a different dataset than in the last exercise sheet. Write a function `find_pop_density(pop, areas, abbreviations)` that takes three dataframes as parameters.\n",
    "\n",
    "**Hint**: The `pop` dataframe contains the population of various abbreviated US states in different years. Additionally, it contains information on whether the count is total or only for people under 18 years. The data can be found in `state-population.csv`. The `areas` dataframe maps the full names of states to their respective area in square miles. The data can be found in `state-areas.csv`. Finally, the `abbreviations` dataframe maps state abbreviations to their full name. The data can be found in `state-abbrevs.csv`. **Your task is to find the mean total population density per state in inhabitants / square kilometer. The conversion factor from square miles to square kilometers is `1 / 0.38610`. You can use inner joins since we don't care about values that don't match in this exercise. Your final result should be a `pd.Series`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f0d214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population density merging test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "from pandas.testing import assert_series_equal\n",
    "pop = pd.read_csv('../data/preprocessing/state-population.csv')\n",
    "areas = pd.read_csv('../data/preprocessing/state-areas.csv')\n",
    "abbreviations = pd.read_csv('../data/preprocessing/state-abbrevs.csv')\n",
    "\n",
    "expected = pd.read_csv('../data/preprocessing/pop_density.csv', index_col=0, header=None).squeeze(1)\n",
    "expected.index.name = 'state'\n",
    "result = find_pop_density(pop, areas, abbreviations)\n",
    "assert isinstance(result, pd.Series), 'Your function needs to return a Series.'\n",
    "assert_series_equal(result, expected, check_names=False)\n",
    "print(\"Population density merging test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f4c7b",
   "metadata": {},
   "source": [
    "## Task 2 - Data Preparation of the Titanic Dataset\n",
    "\n",
    "In tasks a) and b) we explore the Titanic dataset by converting some columns to categorical columns and then calculating some pivot tables. You already know the Titanic dataset from the practical notebook. It contains features about passengers on the tragic Titanic voyage. A detailed description can be found here https://www.kaggle.com/c/titanic/data.\n",
    "\n",
    "In tasks c) and d) we return to the Ebola dataset that we cleaned in Task 1. We will use our new knowledge about time series data to properly index and smooth the dataset.\n",
    "\n",
    "All functions you write that accept a DataFrame should **not** modify the variable passed to them. So it may make sense to first copy the passed DataFrame before performing operations on it.\n",
    "\n",
    "a) **Categorical columns:** The titanic dataset contains columns that can naturally be considered categorical, and others that can be converted to categorical columns by tiling. Write a function `categorize(titanic)` that does the following:\n",
    "\n",
    "1. Convert the `Sex` column to unordered categories.\n",
    "2. Convert `Pclass` and `Embarked` to ordered categories. The order should be 1 < 2 < 3 and S < C < Q (the order in which the Titanic picked up passengers).\n",
    "3. Transform `Parch` (the number of parents or children) into three ordered categories. The first category should contain passengers who had no parents or children on board. The second category those who had 1 or 2, and the last all who had more. Name the categories \"none\", \"few\" and \"many\" respectively.\n",
    "4. Form four ordered categories from the `Age` column based on the four quantiles of the age distribution. Name the categories 'teenage', 'young', 'middleage' and 'senior'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71601ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39427bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5f1b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorization test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "from pandas.testing import assert_frame_equal, assert_series_equal\n",
    "\n",
    "df = pd.read_csv('../data/preprocessing/titanic.csv')\n",
    "df_before = df.copy()\n",
    "result = categorize(df)\n",
    "assert isinstance(result, pd.DataFrame), 'Your function needs to return a DataFrame.'\n",
    "assert set(result[\"Sex\"].cat.categories) == {\"male\", \"female\"}\n",
    "assert set(result[\"Pclass\"].cat.categories) == {1, 2, 3}\n",
    "assert set(result[\"Embarked\"].cat.categories) == {'S', 'C', 'Q'}\n",
    "assert set(result[\"Parch\"].cat.categories) == {'none', 'few', 'many'}\n",
    "assert set(result[\"Age\"].cat.categories) == {'teenage', 'young', 'middleage', 'senior'}\n",
    "\n",
    "assert result[\"Pclass\"].cat.ordered\n",
    "assert result[\"Embarked\"].cat.ordered\n",
    "assert result[\"Parch\"].cat.ordered\n",
    "assert result[\"Age\"].cat.ordered\n",
    "print(\"Categorization test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917130de",
   "metadata": {},
   "source": [
    "b) **Pivot tables:** Write a function `pivot(titanic_cat)` that calculates two pivot tables. The first pivot table should be `Embarked` against `Parch` and contain the mean survival rate. The second pivot table should be `Sex` against `Age`, but contain the **count** of survivors in each of these groups. Consider which aggregation function you can use given how the `Survived` column is encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f4a2f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot tables test passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14805/1038886734.py:3: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  result1 = df.pivot_table(values='Survived', index='Embarked', columns='Parch')\n",
      "/tmp/ipykernel_14805/1038886734.py:4: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  result2 = df.pivot_table(values='Survived', index='Sex', columns='Age', aggfunc=[\"sum\"])\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "result1, result2 = pivot(result)\n",
    "assert isinstance(result1, pd.DataFrame), 'Your function needs to return a DataFrame as first return value.'\n",
    "assert isinstance(result2, pd.DataFrame), 'Your function needs to return a DataFrame as second return value.'\n",
    "\n",
    "assert set(result1.columns.values) == {'none', 'few', 'many'}\n",
    "assert set([n for _, n in result2.columns.values]) == {'teenage', 'young', 'middleage', 'senior'}\n",
    "assert result1.values.max() <= 1 and result1.values.min() >= 0\n",
    "assert result2.values.min() >= 0\n",
    "print(\"Pivot tables test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20325670",
   "metadata": {},
   "source": [
    "c) **Time series indexing:** These tasks focus again on the ebola dataset. Write a function `index_by_time()` that reads `ebola_tidy.csv` from the file system. This is the cleaned version of the ebola dataset we created last time. Use the `Date` as `DatetimeIndex`, i.e., as an index based on timestamps. Then remove the `Day` column and keep only observations made between August 2014 and January 2015 (inclusive). Finally, sort by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52764ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series indexing test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "from datetime import datetime\n",
    "\n",
    "result_c = index_by_time()\n",
    "assert isinstance(result_c, pd.DataFrame), 'Your function needs to return a DataFrame.'\n",
    "\n",
    "assert result_c.index[0] >= datetime(2014, 8, 1)\n",
    "assert result_c.index[-1] <= datetime(2015, 1, 1)\n",
    "print(\"Time series indexing test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eadc32d",
   "metadata": {},
   "source": [
    "d) **Smoothing:** Now we want to look at a smoothed version of the development of the number of Ebola cases in the various countries. First filter out the observations about the number of deaths and keep only those about cases. Then omit the status column. Next, you need to pivot the country names back into columns. Use the `pivot` method for this. It works the same way as `pivot_table`, but doesn't calculate aggregations. Instead, it simply restructures the data. In the next step, finally calculate a moving average over all four weeks, so that the smoothing occurs approximately over a month.\n",
    "\n",
    "**Hint:** On the final result you can now simply call `.plot()` to get a nice visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2258ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0dec9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing test passed!\n",
      "You can call result_d.plot() to visualize the smoothed data!\n"
     ]
    }
   ],
   "source": [
    "# Test case (don't edit this block)\n",
    "result_d = smoothing(result_c)\n",
    "assert isinstance(result_d, pd.DataFrame), 'Your function needs to return a DataFrame.'\n",
    "assert result_d.index[0] >= datetime(2014, 8, 1)\n",
    "assert result_d.index[-1] <= datetime(2015, 1, 1)\n",
    "print(\"Smoothing test passed!\")\n",
    "print(\"You can call result_d.plot() to visualize the smoothed data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bfdc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Basics \n",
    "\n",
    "Next: [**Chapter 4.1: Data Preparation**](../04_ml/01_data_preparation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
