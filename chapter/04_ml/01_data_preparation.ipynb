{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eba2fee",
   "metadata": {},
   "source": [
    "Lecture: AI I - Basics \n",
    "\n",
    "Previous:\n",
    "[**Chapter 3.6: Additional Libraries and Tools**](../03_data/06_additionals.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232848e",
   "metadata": {},
   "source": [
    "# Chapter 4.1: Data Preparation with scikit-learn\n",
    "\n",
    "- [Imputation](#imputation)\n",
    "- [Scaling](#scaling)\n",
    "- [Dimensionality Reduction](#dimensionality-reduction)\n",
    "- [Pipelines](#pipelines)\n",
    "- [Feature Union](#feature-union)\n",
    "- [Column Transformations](#column-transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77899d",
   "metadata": {},
   "source": [
    "__Scikit-learn__ (also known as __sklearn__) is an open-source software library for machine learning in Python. It is very popular and actively maintained. The library offers various classification, regression, and clustering algorithms. In addition, sklearn also includes algorithms for model selection, dimensionality reduction, and data preprocessing.  \n",
    "\n",
    "In this notebook, we (again) focus on data preprocessing (Data Preparation) and cover the following topics:\n",
    "- Imputation  \n",
    "- Scaling  \n",
    "- Dimensionality Reduction  \n",
    "- Pipelines  \n",
    "- Feature Union  \n",
    "- Column Transformations  \n",
    "\n",
    "The documentation for scikit-learn can be found [here](https://scikit-learn.org/stable/index.html).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e8a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb51e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb74b5",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "Imputation refers to the completion of missing values (NaNs). As in pandas, there are various methods in sklearn to replace missing values. More information can be found [here](https://scikit-learn.org/stable/modules/impute.html).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af39251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = np.array([[1, 2], [np.nan, 3], [7, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcba4e",
   "metadata": {},
   "source": [
    "### One-dimensional Imputation\n",
    "\n",
    "In one-dimensional imputation, the values are replaced column by column. The class [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) provides basic strategies for this purpose. Missing values can be replaced with a given constant value or with statistical values (mean, median, or most frequent value) of each column containing the missing values. This class also allows for different encodings of missing values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f991f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# define different imputer\n",
    "mean_imputer = SimpleImputer()\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8366210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [4., 3.],\n",
       "       [7., 6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First the imputer has to be fitted to the data, so either call first fit and then transform \n",
    "# or call fit_transform to do it in one step\n",
    "mean_imputer.fit(nan_data)\n",
    "mean_imputer.transform(nan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b1932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [0., 3.],\n",
       "       [7., 6.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_imputer.fit_transform(nan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0548ad35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.],\n",
       "       [8., 2.],\n",
       "       [6., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_nan_data = np.array([[np.nan, 5], [8, 2], [6, 6]])\n",
    "mean_imputer.transform(different_nan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf51804",
   "metadata": {},
   "source": [
    "__Brainstorming:__  \n",
    "<details>\n",
    "<summary>Why was np.nan replaced with 4?</summary>\n",
    "Because the mean_imputer was previously \"trained\" on the other data.  \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>When can this behavior be an advantage?</summary>\n",
    "An advantage is that the imputation strategies or exact values used during training can also be applied at test time or in live operation.  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2f197f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 5.],\n",
       "       [8., 2.],\n",
       "       [6., 6.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_imputer.transform(different_nan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffa7f2",
   "metadata": {},
   "source": [
    "It is also possible to replace values other than `np.nan`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de1422f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Fischers', 'Fritz', 'fischt', 'frische', 'Fische'],\n",
       "       ['Frische', 'Fische', 'fischt', 'Fischers', 'Fritz']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fischers_fritz = [\n",
    "    ['Fischers', '', 'fischt', 'frische', 'Fische'],\n",
    "    ['Frische', 'Fische', 'fischt', 'Fischers', '']\n",
    "]\n",
    "\n",
    "string_imputer = SimpleImputer(missing_values='', strategy='constant', fill_value='Fritz')\n",
    "string_imputer.fit_transform(fischers_fritz)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5d2a6",
   "metadata": {},
   "source": [
    "### Multidimensional Variant\n",
    "The multidimensional variant is significantly more complex. Roughly summarized, each missing value is modeled as a function of other features, and this estimate is then used for imputation. This process is repeated several times before the final replacements are made. This behavior is implemented by the [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer).  \n",
    "\n",
    "> __Note:__ This class is still experimental.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f488edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38194c4",
   "metadata": {},
   "source": [
    "First, we create a small dummy dataset where the values of the individual columns have a clear relationship to each other.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c766f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.],\n",
       "       [nan,  4.],\n",
       "       [ 3.,  9.],\n",
       "       [ 4., nan],\n",
       "       [ 5., 25.],\n",
       "       [ 6., 36.],\n",
       "       [nan, 49.],\n",
       "       [ 8., 64.],\n",
       "       [ 9., 81.],\n",
       "       [10., nan]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(1, 11, dtype=\"float\")\n",
    "y = x * x \n",
    "\n",
    "data = np.array([x, y])\n",
    "data[(0, 1)] = np.nan\n",
    "data[(0, 6)] = np.nan\n",
    "data[(1, 3)] = np.nan\n",
    "data[(1, 9)] = np.nan\n",
    "data = data.T\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b6ea4",
   "metadata": {},
   "source": [
    "Afterwards, we can again use `fit_transform` to replace the data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0ff6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ],\n",
       "       [ 2.3175117 ,  4.        ],\n",
       "       [ 3.        ,  9.        ],\n",
       "       [ 4.        , 22.35578423],\n",
       "       [ 5.        , 25.        ],\n",
       "       [ 6.        , 36.        ],\n",
       "       [ 6.58808359, 49.        ],\n",
       "       [ 8.        , 64.        ],\n",
       "       [ 9.        , 81.        ],\n",
       "       [10.        , 83.09539114]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IterativeImputer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ff3aa",
   "metadata": {},
   "source": [
    "The replacements of the `SimpleImputer`, on the other hand, look as follows.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "172942e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  1.   ],\n",
       "       [ 5.75 ,  4.   ],\n",
       "       [ 3.   ,  9.   ],\n",
       "       [ 4.   , 33.625],\n",
       "       [ 5.   , 25.   ],\n",
       "       [ 6.   , 36.   ],\n",
       "       [ 5.75 , 49.   ],\n",
       "       [ 8.   , 64.   ],\n",
       "       [ 9.   , 81.   ],\n",
       "       [10.   , 33.625]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleImputer().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8b81a",
   "metadata": {},
   "source": [
    "__Brainstorming:__  \n",
    "<details>\n",
    "    <summary>What are the advantages of the multidimensional variant?</summary>\n",
    "    A major advantage is that the missing data is replaced depending on other data. This is often better, as there may be dependencies between the data. Example: height and weight of individuals.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05ad3c",
   "metadata": {},
   "source": [
    "In the following three lines of code, the iterative working method of the algorithm becomes visible.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2588dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/.venv/lib/python3.12/site-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ],\n",
       "       [ 3.10315829,  4.        ],\n",
       "       [ 3.        ,  9.        ],\n",
       "       [ 4.        , 20.73213454],\n",
       "       [ 5.        , 25.        ],\n",
       "       [ 6.        , 36.        ],\n",
       "       [ 6.8956479 , 49.        ],\n",
       "       [ 8.        , 64.        ],\n",
       "       [ 9.        , 81.        ],\n",
       "       [10.        , 82.62527756]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IterativeImputer(max_iter=1).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283769ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/.venv/lib/python3.12/site-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ],\n",
       "       [ 2.34645245,  4.        ],\n",
       "       [ 3.        ,  9.        ],\n",
       "       [ 4.        , 22.28201783],\n",
       "       [ 5.        , 25.        ],\n",
       "       [ 6.        , 36.        ],\n",
       "       [ 6.61040064, 49.        ],\n",
       "       [ 8.        , 64.        ],\n",
       "       [ 9.        , 81.        ],\n",
       "       [10.        , 83.06934333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IterativeImputer(max_iter=2).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd114ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ],\n",
       "       [ 2.3175117 ,  4.        ],\n",
       "       [ 3.        ,  9.        ],\n",
       "       [ 4.        , 22.35578423],\n",
       "       [ 5.        , 25.        ],\n",
       "       [ 6.        , 36.        ],\n",
       "       [ 6.58808359, 49.        ],\n",
       "       [ 8.        , 64.        ],\n",
       "       [ 9.        , 81.        ],\n",
       "       [10.        , 83.09539114]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IterativeImputer(max_iter=3).fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be53120",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "### Standardization\n",
    "Standardization refers to a transformation of the input data such that the resulting data has a mean of 0 and a variance of 1. The resulting data is therefore normally distributed. This is particularly helpful when all variables are scaled differently.  \n",
    "\n",
    "__Example:__ A dataset contains the values height in meters and weight in kilograms of people. The variance of the variables will clearly differ, since height is on a scale from 0.1 m to 2.8 m, while weight is on a scale from 0.5 kg to 600 kg.  \n",
    "\n",
    "In sklearn, the class [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) is used for standardization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7034cbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittelwerte pro Feature: [  1.84142857 108.61428571]\n",
      "Standardabweichung pro Feature: [ 0.33090476 65.60408151]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaling_data = np.array([\n",
    "    [1.79, 79.5], \n",
    "    [1.60, 53.2], \n",
    "    [2.59, 150], \n",
    "    [1.73, 70.7], \n",
    "    [1.50, 46.7], \n",
    "    [1.75, 113.0], \n",
    "    [1.93, 247.2]\n",
    "])\n",
    "print(f\"Mittelwerte pro Feature: {scaling_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {scaling_data.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27b4f200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15541805, -0.44378772],\n",
       "       [-0.72960139, -0.84467741],\n",
       "       [ 2.26219602,  0.63084054],\n",
       "       [-0.3367391 , -0.57792572],\n",
       "       [-1.03180315, -0.94375661],\n",
       "       [-0.27629875,  0.06685124],\n",
       "       [ 0.26766441,  2.11245568]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(scaling_data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e01c6841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittelwerte pro Feature: [-3.17206578e-17  6.34413157e-17]\n",
      "Standardabweichung pro Feature: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mittelwerte pro Feature: {scaled_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {scaled_data.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445b65a",
   "metadata": {},
   "source": [
    "__Brainstorming:__  \n",
    "<details>\n",
    "<summary>Why isnâ€™t the mean at 0?</summary>\n",
    "These are calculation errors that can be neglected. e-17 is an extremely small number.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed66add",
   "metadata": {},
   "source": [
    "If the data should not be centered, this can be prevented with the parameter `with_mean=False`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f81a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittelwerte pro Feature: [5.56482953 1.65560257]\n",
      "Standardabweichung pro Feature: [1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.40941148, 1.21181485],\n",
       "       [4.83522814, 0.81092516],\n",
       "       [7.82702555, 2.28644311],\n",
       "       [5.22809043, 1.07767685],\n",
       "       [4.53302638, 0.71184595],\n",
       "       [5.28853078, 1.72245381],\n",
       "       [5.83249394, 3.76805824]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_center_scaler = StandardScaler(with_mean=False)\n",
    "non_centric_data = not_center_scaler.fit_transform(scaling_data)\n",
    "\n",
    "print(f\"Mittelwerte pro Feature: {non_centric_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {non_centric_data.std(axis=0)}\")\n",
    "non_centric_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97e6cb",
   "metadata": {},
   "source": [
    "It is also possible not to scale the data in order to preserve the variance. For this, the constructor must be called with the parameter `with_std=False`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0efbe606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittelwerte pro Feature: [0. 0.]\n",
      "Standardabweichung pro Feature: [ 0.33090476 65.60408151]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.14285714e-02, -2.91142857e+01],\n",
       "       [-2.41428571e-01, -5.54142857e+01],\n",
       "       [ 7.48571429e-01,  4.13857143e+01],\n",
       "       [-1.11428571e-01, -3.79142857e+01],\n",
       "       [-3.41428571e-01, -6.19142857e+01],\n",
       "       [-9.14285714e-02,  4.38571429e+00],\n",
       "       [ 8.85714286e-02,  1.38585714e+02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_scaling_scaler = StandardScaler(with_std=False)\n",
    "non_scaled_data = not_scaling_scaler.fit_transform(scaling_data)\n",
    "\n",
    "print(f\"Mittelwerte pro Feature: {non_scaled_data.mean(axis=0)}\")\n",
    "print(f\"Standardabweichung pro Feature: {non_scaled_data.std(axis=0)}\")\n",
    "non_scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ca41e",
   "metadata": {},
   "source": [
    "### Scaling features to a specific range\n",
    "\n",
    "Alternatively, features can be scaled so that they lie between a given minimum and maximum value, often between zero and one, or such that the maximum absolute value of each feature is scaled to unit size. This can be achieved with [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#) or [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8be1fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  ],\n",
       "       [0.25, 0.25],\n",
       "       [0.5 , 0.5 ],\n",
       "       [1.  , 1.  ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_data = np.array([\n",
    "    [-1, 2], \n",
    "    [-0.5, 6], \n",
    "    [0, 10], \n",
    "    [1, 18]\n",
    "])\n",
    "\n",
    "zero_one_scaler = MinMaxScaler()\n",
    "zero_one_scaler.fit_transform(min_max_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfc78f",
   "metadata": {},
   "source": [
    "To change the range, the parameter `feature_range` can be specified in the constructor.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8fce020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. , -1. ],\n",
       "       [-0.5, -0.5],\n",
       "       [ 0. ,  0. ],\n",
       "       [ 1. ,  1. ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minus_one_one_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "minus_one_one_scaler.fit_transform(min_max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d92c6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "max_abs_data = np.array([\n",
    "    [ 1., -1.,  2.],\n",
    "    [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]\n",
    "])\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "max_abs_scaler.fit_transform(max_abs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae6b8a",
   "metadata": {},
   "source": [
    "### Problems with Outliers\n",
    "The three previously introduced classes are not particularly good at handling outliers during scaling. This problem is illustrated [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py).  \n",
    "\n",
    "A scaler that works with outliers is the [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95a2794b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57142857,  0.66666667],\n",
       "       [ 0.57142857, -0.0952381 ],\n",
       "       [-1.14285714, -0.57142857],\n",
       "       [13.71428571,  0.0952381 ],\n",
       "       [ 0.        ,  0.19047619],\n",
       "       [ 0.57142857, -0.95238095],\n",
       "       [-1.14285714,  8.        ],\n",
       "       [ 0.        , -0.85714286],\n",
       "       [ 0.        ,  0.57142857],\n",
       "       [-0.57142857, -0.38095238]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "outlier_data = np.array([\n",
    "    [2, 4, 1, 27, 3, 4, 1, 3, 3, 2],\n",
    "    [100, 92, 87, 94, 95, 83, 177, 84, 99, 89]\n",
    "]).T\n",
    "\n",
    "robust_scaler = RobustScaler(quantile_range=(25, 75))\n",
    "robust_scaler.fit_transform(outlier_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98e901be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.75, 10.5 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "350a35b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., 93.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_scaler.center_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a48b8b",
   "metadata": {},
   "source": [
    "In comparison, the StandardScaler would scale the data as follows.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4ee4d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40525742,  0.        ],\n",
       "       [-0.13508581, -0.30477573],\n",
       "       [-0.54034323, -0.49526056],\n",
       "       [ 2.97188775, -0.2285818 ],\n",
       "       [-0.27017161, -0.19048483],\n",
       "       [-0.13508581, -0.64764842],\n",
       "       [-0.54034323,  2.93346637],\n",
       "       [-0.27017161, -0.60955145],\n",
       "       [-0.27017161, -0.03809697],\n",
       "       [-0.40525742, -0.41906662]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(outlier_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024e6a1",
   "metadata": {},
   "source": [
    "__Exercise:__  \n",
    "<details>\n",
    "<summary>How are the new values calculated?</summary>\n",
    "First, the median and the quantiles per column must be determined. Then the new values can be calculated as follows:  \n",
    "\n",
    "$$x_{new} = \\frac{x_{old} - median}{quantile_{upper} - quantile_{lower}}$$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57ef66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Basics \n",
    "\n",
    "Exercise: [**Exercise 4.1: Data Preparation**](../04_ml/exercises/01_data_preparation.ipynb)\n",
    "\n",
    "Next: [**Chapter 4.2: Machine Learning with scikit-learn**](../04_ml/02_machine_learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
